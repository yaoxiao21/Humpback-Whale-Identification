{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test', 'sample_submission.csv', 'train.csv', 'train']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c1c8f3e339a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TensorFlow version:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Keras version:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# import warnings\n",
    "import warnings\n",
    "# filter warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras.optimizers import RMSprop,Adam\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "print('TensorFlow version:', tf.__version__)\n",
    "print('Keras version:', keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test', 'sample_submission.csv', 'train.csv', 'train']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(\"../input\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_IMAGE_PATH = \"../input/train/\"\n",
    "TEST_IMAGE_PATH = \"../input/test/\"\n",
    "TRAINING_DATA='../input/train.csv'\n",
    "IMG_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['new_whale' 'w_0003639' 'w_0003c59' ... 'w_ffa6d42' 'w_ffcf5fe'\n",
      " 'w_ffe8693']\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(TRAINING_DATA)\n",
    "#string to unique int\n",
    "#set unique int value for each unique classes sring.. string to int\n",
    "unique_calsses_value = np.unique(df_train[['Id']].values)\n",
    "print(unique_calsses_value)\n",
    "unique_classes_id_dict = {}\n",
    "unique_id_classes_dict = {}\n",
    "for i in range(len(unique_calsses_value)):\n",
    "    unique_classes_id_dict[unique_calsses_value[i]] = i\n",
    "    unique_id_classes_dict[i] = unique_calsses_value[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>Id</th>\n",
       "      <th>classes_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000e88ab.jpg</td>\n",
       "      <td>w_f48451c</td>\n",
       "      <td>4786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001f9222.jpg</td>\n",
       "      <td>w_c3d896a</td>\n",
       "      <td>3808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00029d126.jpg</td>\n",
       "      <td>w_20df2c5</td>\n",
       "      <td>662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00050a15a.jpg</td>\n",
       "      <td>new_whale</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0005c1ef8.jpg</td>\n",
       "      <td>new_whale</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Image         Id  classes_id\n",
       "0  0000e88ab.jpg  w_f48451c        4786\n",
       "1  0001f9222.jpg  w_c3d896a        3808\n",
       "2  00029d126.jpg  w_20df2c5         662\n",
       "3  00050a15a.jpg  new_whale           0\n",
       "4  0005c1ef8.jpg  new_whale           0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add new class_id col in df_train df\n",
    "df_train['classes_id'] = df_train.apply (lambda row: unique_classes_id_dict.get(row['Id']),axis=1)\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareImages(train, shape, path):    \n",
    "    x_train = np.zeros((shape, 100, 100, 3))\n",
    "    count = 0\n",
    "    \n",
    "    for fig in train['Image']:\n",
    "        \n",
    "        #load images into images of size 100x100x3\n",
    "        img = image.load_img(\"../input/\"+path+\"/\"+fig, target_size=(100, 100, 3))\n",
    "        x = image.img_to_array(img)\n",
    "        x = preprocess_input(x)\n",
    "\n",
    "        x_train[count] = x\n",
    "        if (count%500 == 0):\n",
    "            print(\"Processing image: \", count+1, \", \", fig)\n",
    "        count += 1\n",
    "    \n",
    "    return x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    w_f48451c\n",
       "1    w_c3d896a\n",
       "2    w_20df2c5\n",
       "3    new_whale\n",
       "4    new_whale\n",
       "Name: Id, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# put labels into y_train variable\n",
    "y_train = df_train[\"Id\"]\n",
    "# Drop 'Id' column\n",
    "X_train = df_train.drop(labels = [\"Id\"], axis = 1)\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image:  1 ,  0000e88ab.jpg\n",
      "Processing image:  501 ,  04c72257b.jpg\n",
      "Processing image:  1001 ,  09cacb84d.jpg\n",
      "Processing image:  1501 ,  0ef961892.jpg\n",
      "Processing image:  2001 ,  141b56a1a.jpg\n",
      "Processing image:  2501 ,  199a417aa.jpg\n",
      "Processing image:  3001 ,  1ec170983.jpg\n",
      "Processing image:  3501 ,  23f084b93.jpg\n",
      "Processing image:  4001 ,  29163ad0b.jpg\n",
      "Processing image:  4501 ,  2e0fab120.jpg\n",
      "Processing image:  5001 ,  3347515d9.jpg\n",
      "Processing image:  5501 ,  3842d71dc.jpg\n",
      "Processing image:  6001 ,  3d7f4c7d5.jpg\n",
      "Processing image:  6501 ,  425f763ca.jpg\n",
      "Processing image:  7001 ,  4714400cd.jpg\n",
      "Processing image:  7501 ,  4c082fbdf.jpg\n",
      "Processing image:  8001 ,  50c683e23.jpg\n",
      "Processing image:  8501 ,  560d986ad.jpg\n",
      "Processing image:  9001 ,  5b68c83ed.jpg\n",
      "Processing image:  9501 ,  60410f111.jpg\n",
      "Processing image:  10001 ,  654951f81.jpg\n",
      "Processing image:  10501 ,  6a572256c.jpg\n",
      "Processing image:  11001 ,  6f96f55b6.jpg\n",
      "Processing image:  11501 ,  74da2b511.jpg\n",
      "Processing image:  12001 ,  7989d9a27.jpg\n",
      "Processing image:  12501 ,  7e5aa2d8a.jpg\n",
      "Processing image:  13001 ,  832382cfb.jpg\n",
      "Processing image:  13501 ,  87f6c0a15.jpg\n",
      "Processing image:  14001 ,  8cfc22e5d.jpg\n",
      "Processing image:  14501 ,  91dcfedcd.jpg\n",
      "Processing image:  15001 ,  97079398e.jpg\n",
      "Processing image:  15501 ,  9c2ad64a9.jpg\n",
      "Processing image:  16001 ,  a11956dff.jpg\n",
      "Processing image:  16501 ,  a5f9ffe86.jpg\n",
      "Processing image:  17001 ,  aaf1a967b.jpg\n",
      "Processing image:  17501 ,  af9a1ffc6.jpg\n",
      "Processing image:  18001 ,  b4e02531d.jpg\n",
      "Processing image:  18501 ,  ba2355ca6.jpg\n",
      "Processing image:  19001 ,  bf60e7fed.jpg\n",
      "Processing image:  19501 ,  c49f39ce3.jpg\n",
      "Processing image:  20001 ,  c960111d0.jpg\n",
      "Processing image:  20501 ,  ce7984d8a.jpg\n",
      "Processing image:  21001 ,  d38efaec9.jpg\n",
      "Processing image:  21501 ,  d831d28ee.jpg\n",
      "Processing image:  22001 ,  dd3ca2387.jpg\n",
      "Processing image:  22501 ,  e288d66cf.jpg\n",
      "Processing image:  23001 ,  e7cc793db.jpg\n",
      "Processing image:  23501 ,  ec8c7229d.jpg\n",
      "Processing image:  24001 ,  f1b850552.jpg\n",
      "Processing image:  24501 ,  f6af8a4b8.jpg\n",
      "Processing image:  25001 ,  fc09f2302.jpg\n"
     ]
    }
   ],
   "source": [
    "x_train = prepareImages(df_train, df_train.shape[0], \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (25361, 100, 100, 3)\n",
      "y_train shape:  (25361, 5005)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train / 255.0\n",
    "print(\"x_train shape: \",x_train.shape)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_train = to_categorical(y_train, num_classes = 5005)\n",
    "print(\"y_train shape: \",y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN model\n",
    "def cnn():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(filters = 16, kernel_size = (5,5), padding = 'Same', activation = 'relu', input_shape = (IMG_SIZE,IMG_SIZE,3)))\n",
    "    model.add(Conv2D(filters = 16, kernel_size = (5,5), padding = 'Same', activation = 'relu'))\n",
    "    model.add(MaxPool2D(pool_size = (2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation = 'relu'))\n",
    "    model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same', activation = 'relu'))\n",
    "    model.add(MaxPool2D(pool_size = (2,2), strides=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))\n",
    "    model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))\n",
    "    model.add(MaxPool2D(pool_size = (2,2), strides=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    # fully connected\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation = 'relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(y_train.shape[1], activation = \"softmax\"))\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 100, 100, 16)      1216      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 100, 100, 16)      6416      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 50, 50, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50, 50, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 50, 50, 32)        4640      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 50, 50, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 25, 25, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 25, 25, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 25, 25, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 25, 25, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               2359552   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5005)              1286285   \n",
      "=================================================================\n",
      "Total params: 3,723,805\n",
      "Trainable params: 3,723,293\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = cnn()\n",
    "optimizer = Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999)\n",
    "model.compile(optimizer = optimizer, loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)\n",
    "epochs = 50\n",
    "batch_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/50\n",
      " - 25s - loss: 8.0136 - acc: 0.2185\n",
      "Epoch 2/50\n",
      " - 17s - loss: 6.9430 - acc: 0.3046\n",
      "Epoch 3/50\n",
      " - 17s - loss: 6.4376 - acc: 0.3263\n",
      "Epoch 4/50\n",
      " - 17s - loss: 6.0647 - acc: 0.3402\n",
      "Epoch 5/50\n",
      " - 17s - loss: 5.7679 - acc: 0.3448\n",
      "Epoch 6/50\n",
      " - 17s - loss: 5.4928 - acc: 0.3511\n",
      "Epoch 7/50\n",
      " - 17s - loss: 5.1903 - acc: 0.3658\n",
      "Epoch 8/50\n",
      " - 17s - loss: 4.9352 - acc: 0.3749\n",
      "Epoch 9/50\n",
      " - 17s - loss: 4.7260 - acc: 0.3801\n",
      "Epoch 10/50\n",
      " - 17s - loss: 4.5619 - acc: 0.3815\n",
      "Epoch 11/50\n",
      " - 17s - loss: 4.3860 - acc: 0.3836\n",
      "Epoch 12/50\n",
      " - 17s - loss: 4.2155 - acc: 0.3854\n",
      "Epoch 13/50\n",
      " - 17s - loss: 4.0400 - acc: 0.3894\n",
      "Epoch 14/50\n",
      " - 17s - loss: 3.8580 - acc: 0.3919\n",
      "Epoch 15/50\n",
      " - 17s - loss: 3.6680 - acc: 0.3996\n",
      "Epoch 16/50\n",
      " - 17s - loss: 3.4734 - acc: 0.4082\n",
      "Epoch 17/50\n",
      " - 17s - loss: 3.2828 - acc: 0.4181\n",
      "Epoch 18/50\n",
      " - 17s - loss: 3.0797 - acc: 0.4320\n",
      "Epoch 19/50\n",
      " - 17s - loss: 2.8643 - acc: 0.4488\n",
      "Epoch 20/50\n",
      " - 17s - loss: 2.6515 - acc: 0.4687\n",
      "Epoch 21/50\n",
      " - 17s - loss: 2.4437 - acc: 0.4969\n",
      "Epoch 22/50\n",
      " - 17s - loss: 2.2183 - acc: 0.5274\n",
      "Epoch 23/50\n",
      " - 18s - loss: 2.0056 - acc: 0.5683\n",
      "Epoch 24/50\n",
      " - 17s - loss: 1.8027 - acc: 0.6049\n",
      "Epoch 25/50\n",
      " - 17s - loss: 1.6089 - acc: 0.6500\n",
      "Epoch 26/50\n",
      " - 17s - loss: 1.4322 - acc: 0.6879\n",
      "Epoch 27/50\n",
      " - 17s - loss: 1.2585 - acc: 0.7313\n",
      "Epoch 28/50\n",
      " - 17s - loss: 1.1031 - acc: 0.7674\n",
      "Epoch 29/50\n",
      " - 17s - loss: 0.9489 - acc: 0.8082\n",
      "Epoch 30/50\n",
      " - 17s - loss: 0.8263 - acc: 0.8360\n",
      "Epoch 31/50\n",
      " - 17s - loss: 0.7137 - acc: 0.8660\n",
      "Epoch 32/50\n",
      " - 17s - loss: 0.6187 - acc: 0.8897\n",
      "Epoch 33/50\n",
      " - 17s - loss: 0.5243 - acc: 0.9117\n",
      "Epoch 34/50\n",
      " - 17s - loss: 0.4562 - acc: 0.9282\n",
      "Epoch 35/50\n",
      " - 17s - loss: 0.3870 - acc: 0.9442\n",
      "Epoch 36/50\n",
      " - 17s - loss: 0.3435 - acc: 0.9539\n",
      "Epoch 37/50\n",
      " - 17s - loss: 0.2951 - acc: 0.9642\n",
      "Epoch 38/50\n",
      " - 17s - loss: 0.2608 - acc: 0.9700\n",
      "Epoch 39/50\n",
      " - 17s - loss: 0.2266 - acc: 0.9764\n",
      "Epoch 40/50\n",
      " - 17s - loss: 0.2016 - acc: 0.9817\n",
      "Epoch 41/50\n",
      " - 17s - loss: 0.1834 - acc: 0.9836\n",
      "Epoch 42/50\n",
      " - 17s - loss: 0.1624 - acc: 0.9871\n",
      "Epoch 43/50\n",
      " - 17s - loss: 0.1472 - acc: 0.9879\n",
      "Epoch 44/50\n",
      " - 17s - loss: 0.1326 - acc: 0.9905\n",
      "Epoch 45/50\n",
      " - 17s - loss: 0.1193 - acc: 0.9922\n",
      "Epoch 46/50\n",
      " - 17s - loss: 0.1099 - acc: 0.9928\n",
      "Epoch 47/50\n",
      " - 17s - loss: 0.1013 - acc: 0.9946\n",
      "Epoch 48/50\n",
      " - 17s - loss: 0.0932 - acc: 0.9944\n",
      "Epoch 49/50\n",
      " - 17s - loss: 0.0858 - acc: 0.9955\n",
      "Epoch 50/50\n",
      " - 17s - loss: 0.0808 - acc: 0.9957\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, verbose=2, callbacks=[learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7960\n",
      "           Image Id\n",
      "0  009dd6c96.jpg   \n",
      "1  2a73b60f7.jpg   \n",
      "2  4d2f1d959.jpg   \n",
      "3  0c9dd9352.jpg   \n",
      "4  f6c4f44c5.jpg   \n",
      "Processing image:  1 ,  009dd6c96.jpg\n",
      "Processing image:  501 ,  e46beacae.jpg\n",
      "Processing image:  1001 ,  092947f89.jpg\n",
      "Processing image:  1501 ,  16af56ec3.jpg\n",
      "Processing image:  2001 ,  4694081ba.jpg\n",
      "Processing image:  2501 ,  af3d0515e.jpg\n",
      "Processing image:  3001 ,  7670d7d5f.jpg\n",
      "Processing image:  3501 ,  d60b30490.jpg\n",
      "Processing image:  4001 ,  37bfa9a13.jpg\n",
      "Processing image:  4501 ,  ad00da0c4.jpg\n",
      "Processing image:  5001 ,  decc807d9.jpg\n",
      "Processing image:  5501 ,  f6d9efbda.jpg\n",
      "Processing image:  6001 ,  e48c9699b.jpg\n",
      "Processing image:  6501 ,  ad0ac1e70.jpg\n",
      "Processing image:  7001 ,  e9cabaa8c.jpg\n",
      "Processing image:  7501 ,  b52b0306d.jpg\n"
     ]
    }
   ],
   "source": [
    "test = os.listdir(\"../input/test/\")\n",
    "print(len(test))\n",
    "col = ['Image']\n",
    "test_data = pd.DataFrame(test, columns=col)\n",
    "test_data['Id'] = ''\n",
    "print(test_data.head())\n",
    "x_test = prepareImages(test_data, test_data.shape[0], \"test\")\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7960/7960 [==============================] - 2s 252us/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(np.array(x_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, pred in enumerate(predictions):\n",
    "    test_data.loc[i, 'Id'] = ' '.join(label_encoder.inverse_transform(pred.argsort()[-5:][::-1]))\n",
    "test_data.head(10)\n",
    "test_data.to_csv('submission_3.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
